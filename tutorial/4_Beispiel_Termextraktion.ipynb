{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dgromann/ProgrammingForTranslators/blob/master/tutorial/4_Beispiel_Termextraktion.ipynb\" target=\"_parent\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pV_GBl7azbZ_"
   },
   "source": [
    "# Programmieren für ÜbersetzerInnen - Beispiel Termextraktion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLczbpNuBE3h"
   },
   "source": [
    "Als erstes Beispiel nach der Einführung schreiben wir ein simples Programm zur Termextraktion unter Verwendung des statistischen Maßes TF-IDF. TF-IDF steht für Termhäufigkeit (TF) und Inverse Dokumentenhäufigkeit (IDF) und ist ein Maß zur Berechnung der Gewichtung eines Wortes/einer Phrase in einem Dokument -  damit können fachsprachliche Benennungen extrahiert werden.\n",
    "\n",
    "Unser Beispielkorpus besteht aus den folgenden drei Sätzen: \n",
    "\n",
    "\n",
    "```\n",
    "document1 = \"Sendung von Gold in einem Container.\" \n",
    "document2= \"Lieferung von Silber in einem silbernen LKW angekommen.\" \n",
    "document3 = \"Sendung von Gold in einem LKW angekommen.\"\n",
    "```\n",
    "Anstatt diese Sätze direkt im Notebook zu schreiben, laden wir drei Dokumente in unser Programm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOhvFdsMzZfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sendung von Gold in einem Container.', 'Lieferung von Silber in einem silbernen LKW angekommen.', 'Sendung von Gold in einem LKW angekommen.']\n"
     ]
    }
   ],
   "source": [
    "# Datein hochladen\n",
    "corpus = []\n",
    "\n",
    "document1 = open(\"document1.txt\")\n",
    "document2= open(\"document2.txt\")\n",
    "document3 = open(\"document3.txt\")\n",
    "\n",
    "documents = [document1, document2, document3]\n",
    "\n",
    "# Die Funktion strip() dient dazu das Symbol \\n für die Markierung des Zeilenendes  \n",
    "# und alle zusätzlichen Leerzeichen vor oder nach der Zeile zu entfernen\n",
    "for document in documents:\n",
    "  for sentence in document.readlines():\n",
    "    corpus.append(sentence.strip())\n",
    "\n",
    "#Wie Sie sehen wird für jedes Dokument eine Liste erstellt, die jede Zeile (in diesem Fall nur eine), des Dokuments enthält.\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6QxQPHJMbZF"
   },
   "source": [
    "Um spaCy auch in diesem Notebook verwenden zu können, müssen wir spaCy und auch das deutsche Sprachpaket wieder laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTeSAXDvhV31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in /srv/conda/envs/notebook/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.4.0,>=7.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (45.1.0.post20200119)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.42.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.1.0)\n",
      "Collecting en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.0MB 84.1MB/s a 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.2)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.1.0.post20200119)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.15.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.1.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Collecting de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 14.9MB 105.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from de_core_news_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (45.1.0.post20200119)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.15.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.1.0)\n",
      "Installing collected packages: de-core-news-sm\n",
      "  Running setup.py install for de-core-news-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed de-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# spaCy in Colab/Binder installieren - für Anaconda siehe spacy Installatiosnanweisungen\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNZMU0S8BhuW"
   },
   "source": [
    "Wir verwenden hier die Vorverarbeitungsmethode aus dem letzten Beispiel. Da diese in einem anderen Notebook steht, müssen wir Sie hier noch einmal wiedergeben: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0ej-1UNjTxC"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocessing(sentence):\n",
    "  sentence = sentence.lower() \n",
    "  # Wir fügen auch gleich ein paar weitere POS-Tags für Konjunktionen, etc. hinzu \n",
    "  pos_to_be_removed =['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "  text_out = []\n",
    "  # Tokenisiert und weitere Vorverarbeitung\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    # POS-Tags überprüft und nur jene die nicht in \"pos_to_be_removed\" zu finden sind berücksichtigen\n",
    "    if token.pos_ not in pos_to_be_removed :\n",
    "      #Lemmatisierung\n",
    "      lemma = token.lemma_\n",
    "      text_out.append(lemma)  \n",
    "  return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5EHpO-LnYtQ"
   },
   "source": [
    "**Aufgabe:** <br>\n",
    "Rufen Sie die oben definierte Methode `preprocessing` für jeden Satz des Korpus `corpus` auf und speichern Sie das Ergebnis in eine neue Liste `preprocessed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-i3QsBhnku_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sendung gold container', 'lieferung silber silbern lkw ankommen', 'sendung gold lkw ankommen']\n"
     ]
    }
   ],
   "source": [
    "# Neue Liste preprocessed \n",
    "preprocessed = []\n",
    "\n",
    "# Schreiben Sie hier Ihre Code - bedenken Sie dass corpus eine Liste ist \n",
    "for sentence in corpus:\n",
    "  prep_sentence = preprocessing(sentence)\n",
    "  preprocessed.append(prep_sentence)\n",
    "\n",
    "# Das Resultat ist eine eigene Liste an Worte für jedes Dokument\n",
    "# Die nachstehende Zeile fügt die einzelnen Worte für jedes Dokument wieder zu Sätzen zusammen\n",
    "preprocessed = [\" \".join(x) for x in preprocessed] \n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jl5zc3egNtzO"
   },
   "source": [
    "# TF-IDF\n",
    "Anstelle der manuellen Berechnung der TF-IDF Werte, verwenden wir eine bereitgestellte Library für maschinelles Lernen names `sklearn`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD1qwThnOL4F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wortindex:  {'sendung': 5, 'gold': 2, 'container': 1, 'lieferung': 3, 'silber': 6, 'silbern': 7, 'lkw': 4, 'ankommen': 0}\n",
      "TF-IDF Matrix: \n",
      "(Dokumentennummer, Termindex) TF-IDF Weight\n",
      "  (0, 5)\t0.5178561161676974\n",
      "  (0, 2)\t0.5178561161676974\n",
      "  (0, 1)\t0.680918560398684\n",
      "  (1, 3)\t0.49047908420610337\n",
      "  (1, 6)\t0.49047908420610337\n",
      "  (1, 7)\t0.49047908420610337\n",
      "  (1, 4)\t0.3730219858594306\n",
      "  (1, 0)\t0.3730219858594306\n",
      "  (2, 5)\t0.5\n",
      "  (2, 2)\t0.5\n",
      "  (2, 4)\t0.5\n",
      "  (2, 0)\t0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed)\n",
    "word_index = vectorizer.vocabulary_\n",
    "\n",
    "print(\"Wortindex: \", word_index)\n",
    "print(\"TF-IDF Matrix: \")\n",
    "print(\"(Dokumentennummer, Termindex) TF-IDF Weight\")\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIjmyj50S68s"
   },
   "source": [
    "Der nachstehende Code extrahiert die `n` wichtigsten Terme aus unserem Mini-Korpus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1nKhThCTU_M"
   },
   "source": [
    "**Frage:** <br>\n",
    "Würden Sie sagen, dass diese Auswahl für diesen Mini-Korpus Sinn macht? Stellen diese Worte tatsächlich wichtige semantische Bestandteile unseres Korpus dar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWIcweEtQy_A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sendung' 'lkw' 'gold']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QP1p8C_U_8GI"
   },
   "source": [
    "Um die wichtigsten extrahieren Termini in einer Datei speichern zu können gehen wir wie folgt vor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMFZ7ovL_7mN"
   },
   "outputs": [],
   "source": [
    "# Speichern Sie die resultierende Datei in Ihrem lokalen Dateiverzeichnis\n",
    "with open('results.txt', 'w') as f:\n",
    "    for item in top_n:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOWR4DDC1QTWsdc0/55lnZt",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Beispiel_Termextraktion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
