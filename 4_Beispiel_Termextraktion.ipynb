{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beispiel_Termextraktion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN25NVaep5+CNhiUeM/G+EJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/ProgrammingForTranslators/blob/master/4_Beispiel_Termextraktion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV_GBl7azbZ_",
        "colab_type": "text"
      },
      "source": [
        "# Programmieren für ÜbersetzerInnen - Beispiel Termextraktion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLczbpNuBE3h",
        "colab_type": "text"
      },
      "source": [
        "Als erstes Beispiel nach der Einführung schreiben wir ein simples Programm zur Termextraktion unter Verwendung des statistischen Maßes TF-IDF. TF-IDF steht für Termhäufigkeit (TF) und Inverse Dokumentenhäufigkeit (IDF) und ist ein Maß zur Berechnung der Gewichtung eines Wortes/einer Phrase in einem Dokument -  damit können fachsprachliche Benennungen extrahiert werden.\n",
        "\n",
        "Unser Beispielkorpus besteht aus den folgenden drei Sätzen: \n",
        "\n",
        "\n",
        "```\n",
        "document1 = \"Sendung von Gold in einem Container.\" \n",
        "document2= \"Lieferung von Silber in einem silbernen LKW angekommen.\" \n",
        "document3 = \"Sendung von Gold in einem LKW angekommen.\"\n",
        "```\n",
        "Anstatt diese Sätze direkt im Notebook zu schreiben, laden wir drei Dokumente in unser Programm:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOhvFdsMzZfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datein in Google Colab laden\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "document1 = open(\"document1.txt\")\n",
        "document2= open(\"document2.txt\") \n",
        "document3 = open(\"document3.txt\")\n",
        "corpus = [document1.readlines(), document2.readlines(), document3.readlines()]\n",
        "\n",
        "#Wie Sie sehen wird für jedes Dokument eine Liste erstellt, die jede Zeile (in diesem Fall nur eine), des Dokuments enthält.\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6QxQPHJMbZF",
        "colab_type": "text"
      },
      "source": [
        "Um spaCy auch in diesem Notebook verwenden zu können, müssen wir spaCy und auch das deutsche Sprachpaket wieder laden:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTeSAXDvhV31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spaCy in Colab/Binder installieren - für Anaconda siehe spacy Installatiosnanweisungen\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNZMU0S8BhuW",
        "colab_type": "text"
      },
      "source": [
        "Wir verwenden hier die Vorverarbeitungsmethode aus dem letzten Beispiel. Da diese in einem anderen Notebook steht, müssen wir Sie hier noch einmal wiedergeben: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ej-1UNjTxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(sentence):\n",
        "  sentence = sentence.lower() \n",
        "  # Wir fügen auch gleich ein paar weitere POS-Tags für Konjunktionen, etc. hinzu \n",
        "  pos_to_be_removed =['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
        "  text_out = []\n",
        "  # Tokenisiert und weitere Vorverarbeitung\n",
        "  doc= nlp(sentence)\n",
        "  for token in doc:\n",
        "    # POS-Tags überprüft und nur jene die nicht in \"pos_to_be_removed\" zu finden sind berücksichtigen\n",
        "    if token.pos_ not in pos_to_be_removed :\n",
        "      #Lemmatisierung\n",
        "      lemma = token.lemma_\n",
        "      text_out.append(lemma)\n",
        "    return text_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5EHpO-LnYtQ",
        "colab_type": "text"
      },
      "source": [
        "**Aufgabe** \\\\\n",
        "Rufen Sie die oben definierte Methode `preprocessing` für jeden Satz des Korpus `corpus` auf und speichern Sie das Ergebnis in eine neue Liste `preprocessed`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-i3QsBhnku_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b88dfa02-d0e8-4d41-f64e-4f4fad70f04b"
      },
      "source": [
        "# neue Liste preprocessed \n",
        "preprocessed = []\n",
        "\n",
        "for sentence in corpus: \n",
        "  print(sentence)\n",
        "  preprocessing(sentence)\n",
        "# jedes Dokument (Satz) in der Variable corpus durch die Metehodee preprocessing bearbeiten\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sendung von Gold in einem Container.\\n']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-49d564f172f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# jedes Dokument (Satz) in der Variable corpus durch die Metehodee preprocessing bearbeiten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f3e8a624c3ff>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Wir fügen auch gleich ein paar weitere POS-Tags für Konjunktionen, etc. hinzu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpos_to_be_removed\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ADV'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRON'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CCONJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PUNCT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PART'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ADP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SPACE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtext_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl5zc3egNtzO",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF\n",
        "Anstelle der manuellen Berechnung der TF-IDF Werte, verwenden wir eine bereitgestellte Library für maschinelles Lernen names `sklearn`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD1qwThnOL4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "word_index = vectorizer.vocabulary_\n",
        "\n",
        "print(\"Wortindex: \", word_index)\n",
        "print(\"TF-IDF Matrix: \")\n",
        "print(\"(Dokumentennummer, Termindex) TF-IDF Weight\")\n",
        "print(tfidf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIjmyj50S68s",
        "colab_type": "text"
      },
      "source": [
        "Der nachstehende Code extrahiert die `n` wichtigsten Terme aus unserem Mini-Korpus. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1nKhThCTU_M",
        "colab_type": "text"
      },
      "source": [
        "**Frage** \\\\\n",
        "Würden Sie sagen, dass diese Auswahl für diesen Mini-Korpus Sinn macht? Stellen diese Worte tatsächlich wichtige semantische Bestandteile unseres Korpus dar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWIcweEtQy_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_array = np.array(vectorizer.get_feature_names())\n",
        "tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
        "\n",
        "n = 3\n",
        "top_n = feature_array[tfidf_sorting][:n]\n",
        "print(top_n)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}