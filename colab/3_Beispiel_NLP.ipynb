{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Beispiel_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPh0W2BmbS8/i3mIdre1x31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/ProgrammingForTranslators/blob/master/colab/3_Beispiel_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRFg2iThENEV",
        "colab_type": "text"
      },
      "source": [
        "# Programmieren für ÜbersetzerInnen - Beispiel NLP\n",
        "\n",
        "NLP steht für Natural Language Processing (NLP) und stellt in allen Übersetzungs-, Konkordanz-, und Termextraktionstools meist wichtige Verarbeitungsschritte zur Verfügung. \n",
        "\n",
        "Typische NLP-Schritte sind: \n",
        "\n",
        "\n",
        "*   Kleinschreibung: um eine bessere Vergleichbarkeit von Texten zu erreichen wird oft der gesamte Text in eine Kleinschreibweise umgewandelt\n",
        "*   Tokenisierung: Trennen von längeren Sequenzen in einzelnen Elemente, z. B. Wörter und Interpunktion \n",
        "*   Lemmatisierung: Trennen von längeren Sequenzen in einzelnen Elemente, z. B. Wörter und Interprunktion \n",
        "*   POS-Tagging: Automatisches Erkennen von Wortklassen \n",
        "*   Stoppwort-Entfernung: Häufig werden Worte die wenig semantisches Gewicht für den Inhalt des Textes haben, aber sehr häufig auftreten, z. B. Pronomen, Artikel, Präpositionen, etc. entfernt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbeC_nBuEjS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unser Beispielsatz\n",
        "sentence = \"Die Bürsten mit den schwarzen Borsten bürsten besser, als die Bürsten mit den weißen Borsten bürsten!\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2YZKNZU0cRI",
        "colab_type": "text"
      },
      "source": [
        "Um das Rad nicht neu zu erfinden und nicht all diese Funktionen selbst zu schreiben, verwenden wir wo verfügbar vordefinierte Funktionen. Vordefinierte Funktionen in Python können einfach so wie eine selbstdefinierte Funktion verwendet werden. \n",
        "\n",
        "Für alle Funktionen, die aus einer Funktionssammlung, einer sogenannten Bibliothek (library) kommen, muss die entsprechende Bibliothek noch installiert und in das jeweilige Programm (hier Notebook) importiert werden. \n",
        "\n",
        "Wir verwenden hier heute die Bibliothek SpaCy (https://spacy.io/api), da diese im Gegensatz zu vielen anderen Bibliotheken eine gute Unterstützung der deutschen Sprache bietet. Eine Installattionsanleitung für zuhause ist hier zu finden: https://spacy.io/usage \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtRLD-f3XMdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spaCy in Colab/Binder installieren - für Anaconda siehe spacy Installatiosnanweisungen\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-sUZnkTp3Hk",
        "colab_type": "text"
      },
      "source": [
        "# Kleinschreiben\n",
        "Der erste Schritt besteht meist darin, alle Worte auf eine Kleinschreibweise zu verwandeln. Dazu kann man die vordefinierte Funktion `lower()` in Python verwenden. Genau wie bei den Funktionen die wir selbst geschrieben haben, dienen die Klammern dazu anzuzeigen, dass es sich um eine Funktion und nicht eine Variable handelt. \n",
        "\n",
        "\n",
        "Eine gute Quelle für die Auflistung aller vordefinierten Funktionen in Python ist die Python Dokumentation: [https://docs.python.org/3/library/functions.html](https://docs.python.org/3/library/functions.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0e2ZP4qJvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_lower = sentence.lower()\n",
        "print(sentence_lower)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWwvBjdxqrXZ",
        "colab_type": "text"
      },
      "source": [
        "# Tokenisieren\n",
        "\n",
        "Tokenisieren bezieht sich auf die Auftrennung einer längeren Sequenz in einzelne Elemente, z. B. bei einem Satz bedeutet das einzelne Wörter und Interpunktion. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pH8wYiqq3YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy \n",
        "\n",
        "nlp = spacy.load('de')\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Diese Methode hat den Satz tokenisiert\n",
        "for token in doc: \n",
        "  print(token)\n",
        "\n",
        "# Zum Vergleich hier die Ausgabe wenn man den Originalsatz verwendet \n",
        "for elem in sentence:\n",
        "  print(elem)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzdQy5ETFtMq",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatisieren\n",
        "\n",
        "Um Wörter besser vergleichbar zu machen werden sie auf ihre Basisform (nicht immer unbedingt der Stamm, dafür gibt es Stemming) reduziert. spaCy hat das alles schon für uns erledigt durch diesen einen `nlp(sentence)`-Befehl. Anbei geben wir nur noch das Ergebnis aus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043gV7WyHHyI",
        "colab_type": "text"
      },
      "source": [
        "**Frage:** \\\\\n",
        "Worin konkret unterscheidet sich der Ausgangssatz am Anfang dieses Notebooks von der nachstehenden Version? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_GpCd5EGb1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tabulate import tabulate\n",
        "for token in doc: \n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUjNbbIUIPdg",
        "colab_type": "text"
      },
      "source": [
        "# Part-of-Speech (POS) Tagging\n",
        "\n",
        "Part-of-Speech (POS) tags sind englische Standardbezeichnungen für Wortklassen, die bei spaCy hier definiert werden: https://spacy.io/api/annotation. In dem nachstehenden Beispiel finden Sie etwa: \n",
        "\n",
        "*   DET = Artikel\n",
        "*   NOUN = Substantiv \n",
        "*   ADP = Präposition\n",
        "*   ADJ = Adjektiv oder Adverb\n",
        "*   VERB = Verb\n",
        "*   PUNCT = Interpunktion\n",
        "\n",
        "Die POS-Tags könenn dazu verwendet werden um bestimmte Wortklassen aus dem Korpus zu entfernen, beispielsweise Interpunktion, Präposittionen, und Artikel. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1v4Lo1tIZEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc: \n",
        "  print(token, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_qGNLgXJMpL",
        "colab_type": "text"
      },
      "source": [
        "**Aufgabe:** <br>\n",
        "Schreiben Sie eine Funktion, die einen Satz\n",
        "\n",
        "\n",
        "*   in die Kleinschreibweise verwandelt,\n",
        "*   tokenisiert,\n",
        "*   nach POS filtert und alle Präspositionen, Artikel und Interpunktionselemente entfernt \n",
        "*   lemmatisiert,\n",
        "\n",
        "und dann den bearbeiteten Satz zurückgibt. Testen Sie die Methode mit dem Satz oben und mit einem weiteren Satz, den Sie hier hinzufügen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RxrB1YzJMFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(sentence):\n",
        "  pos_to_be_removed = [\"DET\", \"ADV\", \"PUNCT\"]\n",
        "  text_out = []\n",
        "  \n",
        "  # Ihr Code für die Kleinschreibweise\n",
        "  \n",
        "  # Ihr Code für Tokenisierung, Lemmatisierung, und POS-Filter (alles in einer for-Schleife mit if-Anweisungen)\n",
        "  for token in doc: \n",
        "\n",
        "  return text_out    \n",
        "    \n",
        "    \n",
        "print(preprocessing(sentence))\n",
        "\n",
        "# Entwicklen Sie einen weiteren Testfall um Ihre Methode zu testen - also geben Sie einen zweiten Satz ein\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc-wxrMf30Er",
        "colab_type": "text"
      },
      "source": [
        "# Was spaCy noch so kann...\n",
        "Dependency Parsing: Ein weiteres Beispiel für sehr interessante NLP-Prozesse ist das automatische Erkennen von grammatischen Beziehungen zwischen Worten/Phrasen eines Satzes. Die angezeigten \"Tags\" auf den einzlenen Beziehungen sind hier unter \"Dependency Parsing\" definiert: https://spacy.io/api/annotation#dependency-parsing. Zum Beispiel \"sb\" steht für \"subject\" und \"pd\" steht für \"predicate\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feGmg4fc37oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('de')\n",
        "doc = nlp(\"Das ist ein Beispiel für die Art der Visualisierung von spacy.\")\n",
        "spacy.displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3OyjxNl5XcP",
        "colab_type": "text"
      },
      "source": [
        "Named entity recognition: erkennen von benannten Entitäten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FTO2H9K5dJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "print(\"Text\", \"Startindex\", \"Endindex\", \"Beschreibung\")\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
