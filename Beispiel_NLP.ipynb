{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beispiel_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+re81orgzaQuijmQEJoka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/ProgrammingForTranslators/blob/master/Beispiel_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRFg2iThENEV",
        "colab_type": "text"
      },
      "source": [
        "# Programmieren für ÜbersetzerInnen - Beispiel NLP\n",
        "\n",
        "NLP steht für Natural Language Processing (NLP) und stellt in allen Übersetzungs-, Konkordanz-, und Termextraktionstools meist wichtige Verarbeitungsschritte zur Verfügung. \n",
        "\n",
        "Typische NLP-Schritte sind: \n",
        "\n",
        "\n",
        "*   Kleinschreibung: um eine bessere Vergleichbarkeit von Texten zu erreichen wird oft der gesamte Text in eine Kleinschreibweise umgewandelt\n",
        "*   Tokenisierung: Trennen von längeren Sequenzen in einzelnen Elemente, z. B. Wörter und Interprunktion \n",
        "*   Lemmatisierung: Trennen von längeren Sequenzen in einzelnen Elemente, z. B. Wörter und Interprunktion \n",
        "*   POS-Tagging: Automatisches Erkennen von Wortklassen \n",
        "*   Stoppwort-Entfernung: Häufig werrden Worte die wenig semantisches Gewicht für den Inhalt des Textes haben, aber sehr häufig auftreten, z. B. Pronomen, Artikel, Präpositionen, etc. entfernt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbeC_nBuEjS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unser Beispielsatz\n",
        "sentence = \"Die Bürsten mit den schwarzen Borsten bürsten besser, als die Bürsten mit den weißen Borsten bürsten!\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2YZKNZU0cRI",
        "colab_type": "text"
      },
      "source": [
        "Um das Rad nicht neu zu erfinden und nicht all diese Funktionen selbst zu schreiben, verwenden wir wo verfügbar vordefinierte Funktionen. Vordefinierte Funktionen in Python können einfach so wie eine selbstdefinierte Funktion verwendet werden. \n",
        "\n",
        "Alle anderen Funktionen, die aus einer Funktionssammlung, einer sogenannten Bibliothek (library) kommen, muss die entsprechende Bibliothek noch installiert und in das jeweilige Programm (hier Notebook) importiert werden. \n",
        "\n",
        "Wir verwenden hier heute die Bibliothek SpaCy (https://spacy.io/api), da diese im Gegensatz zu vielen anderen Bibliotheken eine gute Unterstützung der deutschen Sprache bietet. Eine Installattionsanleitung für zuhause ist hier zu finden: https://spacy.io/usage \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-sUZnkTp3Hk",
        "colab_type": "text"
      },
      "source": [
        "# Kleinschreiben\n",
        "Der erste Schritt besteht meist darin, alle Worte auf eine Kleinschreibweise zu verwandeln. Dazu kann man die vordefinierte Funktion `lower()` in Python verwenden. Genau wie bei den Funktionen die wir selbst geschrieben haben, dienen die Klammern dazu anzuzeigen, dass es sich um eine Funktion und nicht eine Variable handelt. \n",
        "\n",
        "\n",
        "Eine gute Quelle für die Auflistung aller vordefinierten Funktionen in Python ist die Python Dokumentation: [https://docs.python.org/3/library/functions.html#open](https://docs.python.org/3/library/functions.html#open)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0e2ZP4qJvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_lower = sentence.lower()\n",
        "print(sentence_lower)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWwvBjdxqrXZ",
        "colab_type": "text"
      },
      "source": [
        "# Tokenisieren\n",
        "\n",
        "Tokenisieren bezieht sich auf die Auftrennung einer längeren Sequenz in einzelne Elemente, z. B. bei einem Satz bedeutet das einzelne Wörter und Interpunktion. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pH8wYiqq3YO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "9ac40d30-4a4a-4377-dffa-69cfc312de55"
      },
      "source": [
        "import spacy \n",
        "\n",
        "nlp = spacy.load('de_core_news_sm')\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Diese Methode hat den Satz tokenisiert\n",
        "for token in doc: \n",
        "  print(token)\n",
        "\n",
        "# Zum Vergleich hier die Ausgabe wenn man den Originalsatz verwendet \n",
        "for elemen in sentence:\n",
        "  print(elem)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5d8e09bdd71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'de_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzdQy5ETFtMq",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatisieren\n",
        "\n",
        "Um Wörter besser vergleichbar zu machen werden sie auf ihre Basisform (nicht immer unbedingt der Stamm, dafür gibt es Stemming) reduziert. spaCy hat das alles schon für uns erledigt durch diesen einen `nlp(sentence)`-Befehl. Anbei geben wir nur noch das Ergebnis aus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043gV7WyHHyI",
        "colab_type": "text"
      },
      "source": [
        "**Frage:** \\\\\n",
        "Worin konkret unterscheidet sich der Ausgangssatz am Anfang dieses Notebooks von der nachstehenden Version? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_GpCd5EGb1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc: \n",
        "  print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUjNbbIUIPdg",
        "colab_type": "text"
      },
      "source": [
        "# Part-of-Speech (POS) Tagging\n",
        "\n",
        "Part-of-Speech (POS) tags sind englische Standardbezeichnungen für Wortklassen, die bei spaCy hier definiert werden: https://spacy.io/api/annotation. In dem nachstehenden Beispiel finden Sie etwa: \n",
        "\n",
        "*   DET = Artikel\n",
        "*   NOUN = Substantiv \n",
        "*   ADP = Präposition\n",
        "*   ADJ = Adjektiv oder Adverb\n",
        "*   VERB = Verb\n",
        "*   PUNCT = Interpunktion\n",
        "\n",
        "Die POS-Tags könenn dazu verwendet werden um bestimmte Wortklassen aus dem Korpus zu entfernen, beispielsweise Interpunktion, Präposittionen, und Artikel. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1v4Lo1tIZEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc: \n",
        "  print(token.pos_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_qGNLgXJMpL",
        "colab_type": "text"
      },
      "source": [
        "**Aufgabe** \\\\\n",
        "Schreiben Sie eine Funktion, die einen Satz\n",
        "\n",
        "\n",
        "*   in die Kleinschreibweise verwandelt,\n",
        "*   tokenisiert,\n",
        "*   nach POS filtert und alle Präspositionen, Artikel und Interpunktionselemente entfernt \n",
        "*   lemmatisiert,\n",
        "\n",
        "und dann den bearbeiteten Satz zurückgibt. Testen Sie die Methode mit dem Satz oben und mit einem weiteren Satz, den Sie hier hinzufügen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RxrB1YzJMFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocessing(sentence): \n",
        "  pos_to_be_removed = [\"DET\", \"ADV\", \"PUNCT\"]\n",
        "  text_out = []\n",
        "  # Ihr Code für die Kleinschreibweise\n",
        "\n",
        "  # Ihr Code für Tokenisierung, Lemmatisierung, und POS-Filter (alles in einer for-Schleife mit if-Anweisungen)\n",
        "  for token in doc: \n",
        "\n",
        "\n",
        "\n",
        "print(preprocessing(sentence))\n",
        "\n",
        "# Entwicklen Sie einen weiteren Testfall um Ihre Methode zu testen - also geben Sie einen zweiten Satz ein\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc-wxrMf30Er",
        "colab_type": "text"
      },
      "source": [
        "# Was spaCy noch so kann...\n",
        "Dependency Parsing: Ein weiteres Beispiel für sehr interessante NLP-Prozesse ist das automatische Erkennen von grammatischen Beziehungen zwischen Worten/Phrasen eines Satzes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feGmg4fc37oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"displaCy uses JavaScript, SVG and CSS.\")\n",
        "spacy.displacy.serve(doc, style='dep')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3OyjxNl5XcP",
        "colab_type": "text"
      },
      "source": [
        "Named entity recognition: erkennen von benannten Entitäten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FTO2H9K5dJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(\"Text\", \"Startindex\", \"Endindex\", \"Beschreibung\")\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}